\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,bm}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{cleveref}       % smart cross-referencing
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{bm}             % bold fonts for math symbols
\usepackage[misc]{ifsym}    % collection of symbols
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}          % force figure positioning in text
\usepackage{chngcntr}       % counters per section
\usepackage{tikz}
\usepackage{pgfplots,etoolbox}
\usepackage{comment}
\usetikzlibrary{external}
\usepgfplotslibrary{dateplot, groupplots, fillbetween}
\pgfplotsset{compat=newest}
\tikzexternalize[prefix=cache/]

\usepackage{plotpolicies}   % custom package to plot policy-executions

% default \cite to use parentheses
\renewcommand{\cite}[1]{\citep{#1}}

\newcommand{\foi}{\lambda}
\newcommand\todo[1]{\textcolor{red}{(TODO: #1)}}

%MDP
%state space
\newcommand{\mdpstatespace}{\mathcal{S}}
%action space
\newcommand{\mdpactionspace}{\mathcal{A}}
%mdp discount factor
\newcommand{\mdpdiscount}{\gamma}
%mdp policy
\newcommand{\mdppolicy}{\pi}
%mdpvaluefunction
\newcommand{\mdpvaluefunction}{V}
%vectorial value function
\newcommand{\momdpvaluefunction}{\mathbf{V}}
%mdpqfunction
\newcommand{\mdpqfunction}{Q}
%mdp q function estimate
\newcommand{\mdpqfunctionestimate}{\hat{Q}}
%mdp optimal q function
\newcommand{\mdpoptimalqfunction}{Q^{*}}
%q step size
\newcommand{\qstepsize}{\alpha}
%\advantageestimate
\newcommand{\advantageestimate}{\hat{A}}
%transition function
\newcommand{\mdptransition}{\mathcal{T}}
%reward fn
\newcommand{\mdprewardfn}{\mathcal{R}}
%vectorial reward fn
\newcommand{\momdprewardfn}{\bm{\mathcal{R}}}
%mdp state
\newcommand{\mdpstate}{\mathbf{s}}
%mdp reward
\newcommand{\mdpreward}{r}
\newcommand{\momdpreward}{\mathbf{r}}
\newcommand{\momdpreturn}{\mathbf{R}}
%mdp action
\newcommand{\action}{a}
%vectorial action
\newcommand{\mdpaction}{\mathbf{a}}
% the 6 social contact matrices
\newcommand{\ctotal}{C}
\newcommand{\chome}{C_{\text{home}}}
\newcommand{\cwork}{C_{\text{work}}}
\newcommand{\ctransport}{C_{\text{transport}}}
\newcommand{\cschool}{C_{\text{school}}}
\newcommand{\cleisure}{C_{\text{leisure}}}
\newcommand{\cother}{C_{\text{other}}}
% tentative momdp env name
\newcommand{\momdpname}{MOBelCov}
%other params
\newcommand{\proportionalityfactor}{q}
\newcommand{\agegroup}{k}
\newcommand{\agegroups}{K}
\newcommand{\budget}{\bm{b}}

\newcommand{\ltuple}{\langle}
\newcommand{\rtuple}{\rangle}


% color scheme for plots: plot-color-i
\definecolor{pc1}{RGB}{76, 120, 168}
\definecolor{pc2}{RGB}{245, 133, 24}
\definecolor{pc3}{RGB}{228, 87, 88}
\definecolor{pc4}{RGB}{114, 183, 178}
\definecolor{pc5}{RGB}{84, 162, 75}
\definecolor{pc6}{RGB}{238, 202, 59}
\definecolor{pc7}{RGB}{178, 121, 162}


\title{Exploring the Pareto front of multi-objective COVID-19 mitigation policies using reinforcement learning}

% Here you can change the date presented in the paper title
%\date{September 9, 1985}
% Or remove it
\date{}

\author{Mathieu Reymond \\
	Vrije Universiteit Brussel\\
	Brussels, Belgium \\
	\texttt{mathieu.reymond@vub.be} \\
	\And
	Conor F. Hayes \\
	National University of Ireland Galway\\
	Galway, Ireland\\
% 	\texttt{c.hayes13@nuigalway.ie} \\
	\And
	Lander Willem \\
	University of Antwerp \\
	Antwerp, Belgium \\
	\And
	Roxana R\u{a}dulescu \\
	Vrije Universiteit Brussel\\
	Brussels, Belgium \\
	\And
	Steven Abrams \\
	Hasselt University \\
	Hasselt, Belgium \\
	\And
	Diederik M.\ Roijers \\
	HU University of Applied Sciences Utrecht \\
	Utrecht, the Netherlands \\
	\And
	Enda Howley \\
	National University of Ireland Galway\\
	Galway, Ireland\\
	\And
	Patrick Mannion \\
	National University of Ireland Galway\\
	Galway, Ireland\\
	\And
	Niel Hens \\
	Hasselt University \\
	Hasselt, Belgium \\
	\And
	Ann Now\'{e} \\
	Vrije Universiteit Brussel\\
	Brussels, Belgium \\
	\And
	Pieter Libin \\
	Vrije Universiteit Brussel\\
	Brussels, Belgium \\
    \texttt{pieter.libin@vub.be} \\
% 	\AND
% 	Coauthor \\
% 	Affiliation \\
% 	Address \\
% 	\texttt{email} \\
}

% \author{Mathieu Reymond\inst{1}{\Letter} \and Conor F. Hayes \inst{4} \and Lander Willem  \inst{6}\and Roxana R\u{a}dulescu \inst{1} \and Steven Abrams \inst{3,7} \and Diederik M.\ Roijers\inst{1,5}\and Enda Howley\inst{4} \and Patrick Mannion\inst{4} \and Niel Hens \inst{3,6} \and Ann Now\'{e}\inst{1} \and Pieter Libin\inst{1,2,3}{\Letter}}

% \authorrunning{M. Reymond et al.}

% \institute{ 
% Artificial intelligence lab, Vrije Universiteit Brussel, Brussels, Belgium \email{\{pieter.libin,mathieu.reymond,diederik.roijers,ann.nowe\}@vub.be} 
% \and
% Rega institute, KU Leuven, Leuven, Belgium 
% \and  
% Data science institute, Hasselt University, Hasselt, Belgium
% \and
% National University of Ireland Galway, Ireland \email{\{c.hayes13\}@nuigalway.ie}
% \and
% HU University of Applied Sciences Utrecht, the Netherlands
% \and
% Centre for Health Economic Research and Modelling Infectious Diseases
% CHERMID, University of Antwerp, Belgium
% \and
% Global Health Institute, University of Antwerp, Belgium
% }

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Exploring the Pareto front of multi-objective COVID-19 mitigation policies}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Exploring the Pareto front of multi-objective COVID-19 mitigation policies using reinforcement learning},
pdfauthor={Mathieu Reymond, Conor F.~Hayes, Lander Willem, Roxana R\u{a}dulescu, Steven Abrams , Diederik M.\ Roijers, Enda Howley, Patrick Mannion, Niel Hens, Ann Now\'{e}, Pieter Libin},
pdfkeywords={Multi-objective reinforcement learning, Epidemic control, COVID-19 epidemic models},
}

\begin{document}
\maketitle

\begin{abstract}
Infectious disease outbreaks can have a disruptive impact on public health and societal processes. As decision making in the context of epidemic mitigation is hard, reinforcement learning provides a methodology to automatically learn prevention strategies in combination with complex epidemic models. Current research focuses on optimizing policies with respect to a single objective, such as the pathogen's attack rate. However, as the mitigation of epidemics involves distinct, and possibly conflicting, criteria (i.a., prevalence, mortality, morbidity, cost), a multi-objective decision approach is warranted to learn balanced policies. To lift this decision-making process to real-world epidemic models, we apply deep multi-objective reinforcement learning and build upon a state-of-the-art algorithm, Pareto Conditioned Networks (PCN), to learn a set of solutions that approximates the Pareto front of the decision problem. We consider the first wave of the Belgian COVID-19 epidemic, which was mitigated by a lockdown, and study different deconfinement strategies, aiming to minimize both COVID-19 cases (i.e., infections and hospitalizations) and the societal burden that is induced by the applied mitigation measures. We contribute a multi-objective Markov decision process that encapsulates the stochastic compartment model that was used to inform policy makers during the COVID-19 epidemic. As these social mitigation measures are implemented in a continuous action space that modulates the contact matrix of the age-structured epidemic model, we extend PCN to this setting. We evaluate the solution set that PCN returns, and observe that it correctly learns to reduce the social burden whenever the hospitalization rates are sufficiently low. In this work, we thus demonstrate that multi-objective reinforcement learning is attainable in complex epidemiological models and provides essential insights to balance complex mitigation policies.
\end{abstract}


% keywords can be removed
\keywords{Multi-objective reinforcement learning  \and Epidemic control \and COVID-19 epidemic models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Infectious disease outbreaks represent a major global challenge \cite{miranda2022tale}. To this end, understanding the complex dynamics that underlie these epidemics is essential. Epidemiological transmission models allow us to capture and understand such dynamics and facilitate the study of prevention strategies through simulation. However, developing efficient mitigation strategies remains a challenging process, given the non-linear and complex nature of epidemics. 

To address these challenges, reinforcement learning provides a methodology to automatically learn mitigation strategies in combination with complex epidemic models~\cite{libin2020}.
Previous research focused on optimising policies with respect to a single objective, such as the pathogen's attack rate, while the mitigation of epidemics is a problem that inherently covers distinct  and possibly conflicting criteria (i.a., prevalence, mortality, morbidity, cost). Therefore, optimizing on a single objective requires that  these distinct criteria are somehow aggregated into a single metric.
Typically, a decision maker will rely on an expert's assistance to manually design such a metric. However, this is a challenging process, as the decision maker is generally not aware of all possible trade-offs, and as such this may result in a metric that exhibits suboptimal performance. Furthermore, manually designing such metrics is time-consuming, costly and error-prone, as this non-intuitive process requires repetitive and tedious tuning to achieve the desired behaviour~\cite{roijers2013survey}. Moreover, taking a single objective approach has several other limiting factors. For example, it reduces the explainability of the learned solution, as we cannot compare the learned behavior with alternatives. Moreover, the preferences of the decision maker might change over time, in which case the optimization process needs to be done again~\cite{hayes2021practical}. 

This challenging process can be circumvented by taking an explicit multi-objective approach that aims to learn the different trade-offs regarding the considered criteria. By assuming that a decision maker will always prefer solutions for which at least one objective improves, it is possible to learn a set of optimal solutions referred to as the \emph{Pareto front}~\cite{hayes2021practical}. This enables decision makers to review each solution on the Pareto front before making a decision, thereby being aware of the trade-offs that each solution implies. 

In this work, we investigate the use of \emph{multi-objective reinforcement learning} (MORL) to learn a set of solutions that approximate the Pareto front of multi-objective epidemic mitigation strategies. We consider the first wave of the Belgian COVID-19 epidemic, which was mitigated by a strict lockdown~\cite{willem2021impact}. When the incidence of confirmed cases was steadily decreasing, epidemiological experts were tasked to investigate deconfinement strategies, to reduce the severe social contact and mobility restrictions.
Here, we consider an epidemiological model that was constructed to describe the Belgian COVID-19 epidemic and was fitted to hospitalisation incidence data and serial sero-prevalence data~\cite{abrams2021modelling}.
This model constitutes a stochastic discrete-time age-structured compartmental model that simulates mitigation strategies by varying social distancing parameters concerning school, work and leisure contacts.  
Based on this model, we contribute a novel mutli-objective epidemiological reinforcement learning environment~(\momdpname), in the form of a multi-objective Markov decision process (MOMDP) \cite{roijers2013survey}. \momdpname\ encapsulates the epidemiological model developed by \citet{abrams2021modelling} to implement state transitions, with an action space that combines a proportional reduction of school, work and leisure contacts at each time step, Furthermore, it defines a reward function based on two objectives: the attack rate (i.e., proportion of the population affected by the pathogen) and the social burden that is induced by the mitigation measures.

To learn and explore the trade-offs between the attack rate and social burden we use a state-of-the-art MORL approach based on Pareto Conditioned Networks (PCN)~\cite{reymond2022pcn}. PCN uses a single neural network to learn the policies that belong to the Pareto front.
As PCN is an algorithm designed for discrete action-spaces, we extend it towards continuous action-spaces to accommodate \momdpname's action-space. With this continuous action variant of PCN, we explore the Pareto front of multi-objective COVID-19 mitigation policies.  

By evaluating the solution set of mitigation policies learned by PCN, we observe that PCN minimises the social burden in scenarios where hospitalization rates are sufficiently low. Therefore, in this work we illustrate that multi-objective reinforcement learning can provide important insights concerning the trade-offs between complex mitigation polices in real-world epidemiological models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
In the following, we use bold notation to denote variables with a vectorial value, while non-bold notation for its scalar counterpart.

\subsection{Multi-Objective Reinforcement Learning}
Real-world decisions problems typically consider multiple and possibly conflicting objectives. Multi-objective reinforcement learning (MORL) can be used to find optimal solutions for sequential decision making problems with multiple objectives~\cite{hayes2021practical}. Multi-objective sequential problems are typically modelled as a multi-objective Markov decision process (MOMDP), i.e., a tuple, $\mathcal{M} = \ltuple\mdpstatespace, \mdpactionspace, \mdptransition, \mdpdiscount, \momdprewardfn\rtuple$, where $\mdpstatespace$, $\mdpactionspace$ denote the state and action spaces respectively, $\mdptransition \colon \mdpstatespace \times \mdpactionspace \times \mdpstatespace  \to \left[ 0, 1 \right]$ denotes a probabilistic transition function, $\mdpdiscount$ is a discount factor determining the importance of future rewards and $\momdprewardfn \colon \mdpstatespace \times \mdpactionspace \times \mdpstatespace \to \mathbb{R}^n$ is an $n$-dimensional vector-valued immediate reward function, where $n$ corresponds to the number of objectives. 

While MOMDPs define the actions an agent can take, it does not define the agent's behavior, i.e., which actions are taken in each state. Given this MOMDP, an agent follows a policy $\mdppolicy$, that expresses the probability to take action $\action \in \mdpactionspace$ when in state $\mdpstate \in \mdpstatespace$: $\mdpstatespace \times \mdpactionspace \to \left[ 0, 1 \right]$. We measure the performance of $\mdppolicy$ through the expected sum of discounted rewards (denoted the \emph{Value}) it achieves from the initial state $\mdpstate_0$ until the end of the decision problem:

\begin{equation}
    \momdpvaluefunction^\mdppolicy = \mathbb{E} \left [ \sum_{t=0}^h \mdpdiscount^t \momdpreward_t |\ \mdppolicy, \mdpstate_0 \right ],
\end{equation}
%
where $\momdpreward_t$ corresponds to the multi-objective reward observed at time $t$, by following policy $\mdppolicy$, given by the reward function $\mdprewardfn(\mdpstate_t, \action_t, \mdpstate_{t+1})$, where $\mdpstate_t$ is the current state, $\action_t$ corresponds to the action that was taken, and $\mdpstate_{t+1}$ signifies the state reached by taking action $\action_t$.

On the one hand, for single-objective RL, where $n = 1$, the goal is to find the policy $\mdppolicy^{*}$ that maximizes the $\mdpvaluefunction$-value:
\begin{equation}
\mdppolicy^*= \arg\max_\mdppolicy \mdpvaluefunction^\mdppolicy.
\end{equation}

On the other hand, for MORL, $n > 1$ which leads to vectorial returns. In this case, there can be policies for which, without any additional information, it is impossible to know if one is better than the other. For example, it is impossible to decide which policy between $\mdppolicy_1, \mdppolicy_2$ is optimal if both policies lead to expected returns $\momdpvaluefunction^{\mdppolicy_1}=(0,1), \momdpvaluefunction^{\mdppolicy_2}=(1,0)$ respectively. We call these solutions \emph{non-dominated}, i.e., solutions for which it is impossible to improve an objective without hampering another. The set that contains all the non-dominated solutions of the decision problem is called the \emph{Pareto front} $\mathcal{F}$. Our goal is to find the set of policies that lead to all the $\mdpvaluefunction$-values contained in the Pareto front $\Pi^* = \{\ \pi | \momdpvaluefunction^{\pi} \in \mathcal{F}\}$. In general, we call any set of $\mdpvaluefunction$-values a \emph{solution set}. When a solution set contains only  non-dominated $\mdpvaluefunction$-values, it is referred to as a \emph{coverage set}. In the case that no $\mdppolicy$ exists that has a $\momdpvaluefunction^{\pi}$ dominating any of the solutions in a coverage set, then this coverage set is the Pareto front.

\subsection{Multi-Objective Metrics}
Comparing the learned coverage sets produced by different algorithms is a non-trivial task, as one algorithm's output might dominate the other in some region of the objective-space, but be dominated in another. Intuitively, one would generally prefer the algorithm that covers a wider range of decision maker preferences. %In this work we use several metrics to evaluate our algorithm's performance.

A widely used metric in the literature is called the \emph{hypervolume}~\cite{zitzler2003}. This metric evaluates the learned coverage set by computing its volume with respect to a fixed reference point. The reference point is taken as a lower bound on the achievable returns so that the volumes are always positive. By definition, the solutions contained in the Pareto front dominate all other possible solutions. Thus, no other solution can further increase the volume under the Pareto front. This means that the hypervolume is the highest for the Pareto front. One drawback of the hypervolume is that it can be difficult to interpret. For example, when working in high-dimensional objective-spaces, adding or removing a single point can drastically change hypervolume values, especially if the point lies close to an extremum of said space.

To counterpoise these shortcomings, we evaluate our work using an additional metric called the $\varepsilon$-indicator $I_\varepsilon$~\cite{zitzler2003}, which measures how close a coverage set is to the Pareto front $\mathcal{F}$. Intuitively, $I_\varepsilon$ shows that any solution of $\mathcal{F}$ is \emph{at most} $\varepsilon$ better with respect to each objective $o$ than the closest solution of the evaluated coverage set:

\begin{equation}
\label{eq:epsilon-metric}
    I_{\varepsilon} = 
    \inf_{\varepsilon\in\mathbb{R}} 
    \{ 
    \forall\ \momdpvaluefunction^\pi\!{\in}\ \mathcal{F}, ~
    \exists\ \momdpvaluefunction^{\pi'}\!{\in}\ \hat{\Pi} :\ 
    || \momdpvaluefunction^\pi - \momdpvaluefunction^{\pi'} ||_{\infty} \le \varepsilon
    \},
\end{equation}
%
where $||.||_{\infty}$, the L-infinity norm, is defined as the magnitude of the largest entry of a vector.

The main disadvantage of this metric is that we need the true Pareto front to compute it, which is unknown for our MOMDP (see Sec.~\ref{sec:sars-cov2-momdp}). To still gain insights from our learned policies, we approximate the true Pareto front using the non-dominated policies across all runs.

We note that the $\varepsilon$-indicator metric is quite pessimistic, as it measures worst-case performance~\cite{zintgraf2015}, i.e., it will still report low performance as long as a single point of the Pareto front is incorrectly modeled, even if all the other points are covered. As such, we also use the $I_{\varepsilon-mean}$ which measures the \emph{average} $\varepsilon$ distance of the solutions in $\mathcal{F}$ with respect to the evaluated coverage set~\cite{reymond2022pcn}.

Fig.~\ref{fig:paretofrontmetrics} shows a visual representation of the hypervolume and $\varepsilon$ metrics in two dimensions.

\begin{figure}[t!]
    \centering
    \begin{tikzpicture}
      \begin{axis}[legend pos=south west, xmin=-1, xmax=4.5, ymin=-0.5, ymax=4.5,ytick={0,1,2,3,4,5},scale only axis,width=0.4\textwidth]
      \addplot[only marks,mark=x,mark size=3] coordinates {(-0.5,0)};
      \addplot[only marks,mark=*] coordinates {(1,4)(2,2)(4,1)};
      \addplot[only marks,mark=o] coordinates {(0.5,3)(0.75,2.3)(2.3,1)(3.3,0.7)};
      % Hypervolume
      \fill [cyan,opacity=0.2] (-0.5,0) -- (-0.5,3) -- (0.5,3) -- (0.5,2.3) -- (0.75,2.3) --
      (0.75,1) -- (2.3,1) -- (2.3,0.7) -- (3.3,0.7) -- (3.3,0) -- cycle;
      % epsilons
      \draw [dashed, -|] (0.5, 3) -- node[right]{$\varepsilon_1$} (0.5,4);
      %\draw [dashed, -|] (0.75,2.3) -- node[below]{x} (2,2.3);
      \draw [dashed, -|] (2.3,1) -- node[right]{$\varepsilon_2$} (2.3,2);
      \draw [dashed, -|] (3.3,0.7) -- node[below]{$\varepsilon_3$} (4,0.7);
      \draw [opacity=0.2,line width=2,orange, dotted] (-0.5,0) -- (-0.5,4) -- (1,4) -- (1,2) --
      (2,2) -- (2,1) -- (4,1) -- (4,0) -- (-0.5,0);
      \end{axis}
    \end{tikzpicture}
    \caption{Example of a Pareto front (black dots) and a coverage set (white dots) in a 2-objective environment. The hypervolume metric (in light blue) measures the volume of all dominated solutions with respect to a reference point (cross). The reference point is taken as a lower bound on the achievable returns, which is why it can differ from the origin. The $\varepsilon$ metrics first compute the maximum distance between each point in the Pareto front and its closest point in the coverage set ($\varepsilon_i$). We can then take their maximum value to compute the $I_\varepsilon$ metric, or their mean value to obtain the $I_{\varepsilon-mean}$ metric of the coverage set.}
    \label{fig:paretofrontmetrics}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{COVID-19 model and \momdpname\  MOMDP}
\label{sec:sars-cov2-momdp}
\subsection{Stochastic compartment model for SARS-CoV-2}
\label{sec:sars-cov2-momdp_model}
To evaluate non-pharmaceutical interventions, we consider the compartmental model presented by Abrams et al., that was used to investigate exit strategies in Belgium after the first epidemic wave of SARS-CoV-2~\cite{abrams2021modelling}. This model concerns a discrete-time stochastic model, that considers an age-structured population. The model generalises a standard SEIR model\footnote{A standard SEIR model divides the population into four different states, i.e., susceptible, exposed, infectious and recovered individuals.}, extended to capture the different stages of disease spread and history that are associated with SARS-CoV-2 (i.e., pre-symptomatic, asymptomatic, symptomatic with mild symptoms and symptomatic with severe symptoms) and to represent the stages associated with severe disease, i.e., hospitalisation, admission to the intensive care unit (ICU) and death.

A visual representation of the model is depicted in Fig.~\ref{fig:compartment_model}. It shows the different compartments, as well as the flow rates at which individuals move between compartments.

These flow rates are defined by a set of ordinary differential equations, which are outlined as follows:

\begin{align*}
\frac{d\textbf{S}(t)}{dt} & = -\lambda(t)(\textbf{S})(t), \\
%
\frac{d\textbf{E}(t)}{dt} & = \lambda(t)(\textbf{S})(t) - \gamma \textbf{E}(t), \\
%
\frac{d\textbf{I}^{presym}(t)}{dt} & = \gamma \textbf{E}(t) - \theta \textbf{I}^{presym}(t), \\
%
\frac{d\textbf{I}^{asym}(t)}{dt} & = \theta p \textbf{I}^{presym}(t) - \delta_{1} \textbf{I}^{asym}(t), \\
%
\frac{d\textbf{I}^{mild}(t)}{dt} & = \theta (1 - p) \textbf{I}^{presym}(t) - \{ \psi + \omega_{2} \} \textbf{I}^{mild}(t), \\
%
\frac{d\textbf{I}^{sev}(t)}{dt} & = \psi \textbf{I}^{mild}(t) - \omega \textbf{I}^{sev}(t), \\
%
\frac{d\textbf{I}^{hosp}(t)}{dt} & = \phi_{1} \omega \textbf{I}^{sev}(t) - \{ \delta_{3} + \tau_{1} \} \textbf{I}^{hosp}(t), \\
%
\frac{d\textbf{I}^{icu}(t)}{dt} & = (1 - \phi_1) \omega \textbf{I}^{sev}(t) - \{ \delta_{4} + \tau_{2} \} \textbf{I}^{icu}(t), \\
%
\frac{d\textbf{D}(t)}{dt} & = \tau_{1} \textbf{I}^{hosp}(t) - \tau_{2} \textbf{I}^{icu}(t), \\
%
\frac{d\textbf{R}(t)}{dt} & = \delta_{1} \textbf{I}^{asym}(t) + \delta_{2} \textbf{I}^{mild}(t) + \delta_{3} \textbf{I}^{hosp}(t) + \delta_{4} \textbf{I}^{icu}(t)
\end{align*}
%

In this set of ordinary differential equations, each state variable represents a vector over all age groups for a particular compartment at time $t$. For example, $\textbf{S} = (S_{1}(t), S_{2}(t), ..., S_{\agegroup}(t))^{T}$ is the vector representing the susceptible members of the population of each age group $\agegroup$ at time $t$. Infection dynamics are governed by an age-specific force of infection $\foi$:

\begin{equation}
    \foi(\agegroup,t) = \sum_{\agegroup'=1}^{\agegroups}\beta(\agegroup,\agegroup')I_{\agegroup'}(t),
\end{equation}
%
where $\agegroups$ is the total number of age groups, and $\beta(\agegroup,\agegroup')$ is the time-invariant transmission rate that encodes the average per capita rate at which an infectious individual in age group $\agegroup$ makes an effective contact with a susceptible individual in age group $\agegroup'$, per unit of time.

As we consider an age-structured population, we consider this extended SEIR structure for $\agegroups = 10$ age groups,  i.e., $[0-10), [10-20), [20-30), [30-40), [40-50), [50-60), [60-70), [70-80), [80-90), [90,100+)$. Contacts of the different age-groups, which impact the propagation rate of the epidemic, are modeled using social contact matrices. We define a social contact matrix for 6 different social environments: $\chome, \cwork, \ctransport, \cschool, \cleisure, \cother$, for the home, work, transport, school, leisure, other environments respectively. The social contact matrix across all social environments is defined as:

\begin{equation}
    \ctotal = \chome + \cwork + \ctransport + \cschool + \cleisure + \cother
\end{equation}

Under the social contact hypothesis \cite{wallinga2006using}, we have that:
%
\begin{equation}
\beta(\agegroup,\agegroup') = \proportionalityfactor \cdot \ctotal(\agegroup,\agegroup'),
\end{equation}
%
where $\proportionalityfactor$ is a proportionality factor.

Following \citet{abrams2021modelling}, we rely on distinct social contact matrices for symptomatic and asymptomatic individuals, respectively $\ctotal_{s}$ and $\ctotal_{a}$. Therefore, we define the transmission rates for both symptomatic and asymptomatic individuals as follows:

\begin{equation}
\boldsymbol{\beta}_{s}(\agegroup, \agegroup) = \proportionalityfactor_{s} \cdot C_{s}(\agegroup, \agegroup'),    
\end{equation}
and
\begin{equation}
\boldsymbol{\beta}_{a}(\agegroup, \agegroup') = \proportionalityfactor_{a} \cdot C_{a}(\agegroup, \agegroup').  
\end{equation}

The age-dependent force of infection can be defined as follows:

\begin{equation}
    \boldsymbol{\lambda}(t) = \boldsymbol{\beta}_{a} \times \{ \textbf{I}^{presym}(t) + \textbf{I}^{asym}(t) \} +
    \boldsymbol{\beta}_{s} \times \{ \textbf{I}^{mild}(t) + \textbf{I}^{sev}(t) \},
\end{equation}
%
where $\boldsymbol{\lambda}(t) = (\lambda(1, t), \lambda(2, t), ..., \lambda(\agegroups, t))$. For all further information about the different compartments and parameters we refer the reader to the work of \citet{abrams2021modelling}.

\todo{epi friends, please check} Time- and age-dependent behavioral changes introduce substantial uncertainty regarding the outcome of an outbreak. To evaluate intervention strategies that modulate such behavioral changes, the use of stochastic epidemiological models is warranted \cite{abrams2021modelling}. Moreover, the effect of stochasticity is most pronounced when studying the initial growth of an epidemic \cite{britton2009epidemic}, be it during its emergence, or when implementing deconfinement strategies.

By formulating the set of differential equations defined above, as a chain-binomial, we can obtain stochastic trajectories from this model \cite{bailey1975mathematical}. A chain-binomial model assumes a stochastic model where infected individuals are generated by some underlying probability distribution. For this stochastic model we consider a time interval $(t, t +h]$, where $h$ is defined as the length between two consecutive time points. In this work we set $h = \frac{1}{240}$.

\subsection{Interventions strategies}
\label{sec:interventions}
To model different types of interventions, we follow the contact reduction scheme presented by \citet{abrams2021modelling}. Firstly, to consider distinct exit scenarios, we modulate the social contact matrices to reflect a contact reduction in a particular age group.

We consider a contact reduction function that imposes a proportional reduction of work (including transport) $p_w$, school $p_s$ and leisure $p_l$ contacts, which is implemented as a linear combination of social contact matrices:
\begin{equation}
\label{eq:scm}
    \hat{C}(p_w, p_s, p_l) = \chome + p_w (\cwork + \ctransport) + p_s \cschool + p_l (\cleisure + \cother)
\end{equation}

We denote $\hat{C}_t$ the social contact matrix at timestep $t$, resulting from the reduction function $\hat{C}$. Secondly, we assume that compliance to the interventions is gradual and model this using a logistic compliance function (see details in Sec.~\ref{sec:compliance-function} of the Appendix).

\subsection{The \momdpname\ Environment}
\begin{figure}[t!]
    \centering
    \begin{tikzpicture}
     \draw (0.0pt, 0.0pt)node[fill=green!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](0){$\textbf{S}$};
     \draw (60.0pt, 0.0pt)node[fill=orange!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](1){$\textbf{E}$};
     \draw (120.0pt, 0.0pt)node[fill=red!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](2){$\textbf{I}^{presym}$};
     \draw (180.0pt, 10.0pt)node[fill=red!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](3){$\textbf{I}^{asym}$};
     \draw (170.0pt, -30.0pt)node[fill=red!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](4){$\textbf{I}^{mild}$};
     \draw (170.0pt, -70.0pt)node[fill=red!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](5){$\textbf{I}^{sev}$};
     \draw (230.0pt, -50.0pt)node[fill=blue!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](6){$\textbf{I}^{hosp}$};
      \draw (75.0pt, -70.0pt)node[fill=gray!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](10){$\textbf{H}^{new}$};
     \draw (230.0pt, -90.0pt)node[fill=blue!20, thick, minimum height=0.6cm,minimum width=0.6cm, draw](7){$\textbf{I}^{icu}$};
     \draw (300.0pt, -50.0pt)node[fill=orange!50, thick, minimum height=0.6cm,minimum width=0.6cm, draw](8){$\textbf{R}$};
     \draw (300.0pt, -90.0pt)node[fill=purple!50, thick, minimum height=0.6cm,minimum width=0.6cm, draw](9){$\textbf{D}$};
     
     %\path[->] (6) edge[dotted, bend right = 50]
     \path[->] (0) edge[] node[above]{$\boldsymbol{\lambda}(t)$} (1);
     \path[->] (1) edge[] node[above]{$\gamma$} (2);
     \path[->] (2) edge[] node[below]{$\textbf{p}\theta$} (3);
     \path[->] (2) edge[] node[near end, below left]{$(1-\textbf{p})\theta$} (4);
     \path[->] (4) edge[] node[left]{$\boldsymbol{\psi}$} (5);
     \path[->] (5) edge[] node[near end, below left]{$(1-\phi_{1})\boldsymbol{\omega}$} (7);
     \path[->] (5) edge[] node[above]{$\phi_{1}\boldsymbol{\omega}$} (6);
     \path[->] (7) edge[] node[near end, below right]{$\boldsymbol{\delta}_{4}$} (8);
     \path[->] (7) edge[] node[above]{$\boldsymbol{\tau}_{2}$} (9);
     \path[->] (6) edge[] node[above]{$\boldsymbol{\delta}_{3}$} (8);
     \path[->] (6) edge[] node[near start, below left]{$\boldsymbol{\tau}_{1}$} (9);
     \path[->] (3) edge[bend left = 30] node[above right]{$\delta_{1}$} (8);
     \path[->] (4) edge[bend left = 30] node[above left]{$\boldsymbol{\delta}_{2}$} (8);
      \path[->] (5) edge[] node[below]{$\textbf{I}^{sev} - \textbf{H}^{new}$} (10);
    \end{tikzpicture}
    
    \caption{Schematic diagram of the compartmental model for SARS-CoV-2 presented by \citet{abrams2021modelling} which is used to derive the MOMDP.}
    \label{fig:compartment_model}
\end{figure}
In order to apply multi-objective reinforcement learning, we construct the \momdpname\ MOMDP based on the epidemiological model introduced in Sec.~\ref{sec:sars-cov2-momdp_model} and graphically depicted in Fig.~\ref{fig:compartment_model}. Moreover, we consider a finite-horizon setting where we simulate the compartment model for a fixed number of weeks.

\paragraph{Action-space:} Our actions concern the installment of a social contact matrix with a particular reduction resulting from the reduction function $\hat{C}$ (see Sec.~\ref{sec:interventions}). To this end, we use the proportional reduction parameters $p_w, p_s, p_l$ defined in Sec.~\ref{sec:interventions}. Thus, each $\mdpaction \in \mdpactionspace$ is a 3-dimensional continuous vector in $[0,1]^3$ (i.e., $\mdpaction = [p_{w}, p_{s}, p_{l}]$) which impacts the social contact matrix according to Eq.~\ref{eq:scm}.

\paragraph{Transition function:} The model defined by \citet{abrams2021modelling} utilises a model transition probability $M$ (see Sec.~\ref{sec:binomial-model} of the Appendix for details on $M$), that progresses the epidemiological model in one timestep based on the currently installed social contact matrix $\hat{C}(p_{w}, p_{s}, p_{l})$.
We use this function as the transition function in \momdpname.

In a classical MDP, executing an action $\mdpaction_t$ in any state $\mdpstate_t$ leads to a next state $\mdpstate_{t+1}$ according to the transition function $\mdptransition$. At every timestep $t$, the agent is free to choose the action to perform. In our case, this potentially results in a different restriction $[p_{w}, p_{s}, p_{l}]$ every week. However, we argue that in the context of mitigation policies, consistency is important and policies that impose changes too frequently will be hard to adhere to.

In order to learn realistic and consistent mitigation policies, we introduce a \emph{budget} regarding the number of times a policy can change its actions until the terminal state of the MOMDP is reached. Concretely, when the action changes, i.e., if the social restriction proposed by the policy is different from the one that is currently in place, we reduce the budget by one. We only allow action changes as long as there is budget left. We note that we can mimic a no-limit budget setting by choosing a budget that corresponds to the horizon of the environment.

Finally, for each timestep $t$, our transition function $\mdptransition$ uses the model transition probability $M$ to simulate the model for one week, using $\hat{C}_t$ obtained from $\mdpaction_t$.

% As mentioned above and detailed in the SI, the compartment model is described by a set of ODEs. To obtain a stochastic version, the ODEs can be formulated as a chain-binomial process. Based on these cases, we also create and experiment on two versions of the \momdpname\ environment: the (deterministic) \emph{ODE model} (i.e., a MOMDP with a deterministic transition function) and the (stochastic) \emph{Binomial model} (i.e., a MOMDP with a stochastic transition function).

\paragraph{State-space:} The state of the MOMDP concerns a 3-tuple. The first element, $\mdpstate_m$, directly corresponds to the aggregation of the state variables in the epidemiological model, i.e., a tuple,
\begin{equation}
    \ltuple S_k, E_k, I_k^{presym}, I_k^{asym}, I_k^{mild}, I_k^{sev}, I_k^{hosp}, I_k^{icu}, H^{new}_k, D_k, R_k \rtuple ,
\end{equation}
%
for each age group $k \in \{1, \ldots, \agegroups\}$, where $S$ encodes the members of the population who are susceptible to infection and $E$ encodes the members of the population who have been exposed to COVID-19. Moreover, $I^{presym}$, $I^{asym}$, $I^{mild}$, $I^{hosp}$, $I^{icu}$ denote the members of the population infected with COVID-19 and are, respectively, presymptomatic, asymptomatic, mildly symptomatic, hospitalised, or in the ICU. Finally, $H^{new}_k$ represents the number of newly hospitalised individuals in age group $k$.

We parameterize the epidemiological model using the mean of the posteriors as specified by \citet{abrams2021modelling} (details in Sec.~\ref{sec:compartment-model-parameters} of the Appendix).

The second element of the tuple consists of the social contact matrix $\hat{C}_t$ that is currently in place. The reason to incorporate it in the state-space is two-fold. First, \citet{abrams2021modelling} define a compliance function, simulating the time people need to get used to the new rules set in place. As such, during the simulated week, there is a gradual shift from the current $\hat{C}_t$ to the new social contact matrix, $\hat{C}_{t+1}$. Thus, we need to include the current $\hat{C}_t$, to maintain a Markovian environment. Secondly, we require the current $\hat{C}_t$ to determine whether the action changes the social restrictions in place, and thus consume part of the budget.

The third element of the tuple concerns the budget $\budget$. We incorporate a distinct budget per action-dimension, so $p_{w}, p_{s}$ and $p_{l}$ each have their own budget, resulting in a vector $\budget = [b_w, b_s, b_l]$. As such, it is possible that, at timestep $t$, the budget for one of the proportional reductions is reduced but not the others.

Therefore, we define a state in \momdpname\ as follows:
\begin{equation}
    \mdpstate = \mdpstate_m \cup \hat{C} \cup \budget
\end{equation}

\paragraph{Reward function:} We define a vectorial reward function which considers multiple objectives: attack rate (i.e., infections and hospitalisations) and the social burden imposed by the interventions on the population.

The attack rate in terms of infections is defined as the difference in susceptibles from the current state to the next state~\cite{libin2020}. Since this is a cost that needs to be minimized, we defined the corresponding reward function as the negative attack rate:
\begin{equation}
\mdprewardfn_{\text{ARI}}(\mdpstate,\mdpaction,\mdpstate') = -(\sum_{k=1}^{\agegroups}S_k(\mdpstate)-\sum_{k=1}^{\agegroups}S_k(\mdpstate')).
\end{equation}

The reward function to reduce the attack rate in terms of hospitalisations is defined as the negative number of new hospitalizations:
\begin{equation}
\mdprewardfn_{\text{ARH}}(\mdpstate,\mdpaction,\mdpstate') = -\sum_{k=1}^{\agegroups}H^\text{new}_k(\mdpstate).
\label{eqn:attack-rate-hosp}
\end{equation}

Finally, we use the missed contacts resulting from the intervention measures as a proxy for societal burden. To quantify missed contacts, we consider the original social contact matrix $C$ and the installed social contact matrix $\hat{C}$ to compute the difference $\hat{C} - C$. 
The resulting difference matrix quantifies the average frequency of contacts missed. To determine missed contacts for the entire population, we apply the difference matrix to the population sizes of the respective age groups that are currently uninfected (i.e., susceptible and recovered individuals). Formally, we define the social burden reward function $\mdprewardfn_{\text{SB}}$, as follows:

\begin{equation}
    \mdprewardfn_{\text{SB}}(\mdpstate,\mdpaction,\mdpstate') = \sum_{i=1}^{\agegroups}\sum_{j=1}^{\agegroups}(\hat{C}-C)_{ij}S_i(\mdpstate)S_j(\mdpstate) + \sum_{i=1}^{\agegroups}\sum_{j=1}^{\agegroups}(\hat{C}-C)_{ij}R_i(\mdpstate)R_j(\mdpstate),
\end{equation}
where $S_k(\mdpstate)$ represents the number of susceptible individuals in age group $k$ in state $\mdpstate$ and $R_k$ represents the number of recovered individuals in age group $k$ in state $\mdpstate$. In Sec.~\ref{sec:experiments}, we optimize PCN on two different variants for the multi-objective reward function: $[\mdprewardfn_\text{ARH}, \mdprewardfn_\text{SB}]$ and $[\mdprewardfn_\text{ARI}, \mdprewardfn_\text{SB}]$, to study the impact of these distinct attack rate quantities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pareto Conditioned Networks}
In multi-objective optimization, the set of optimal policies can grow exponentially in the number of objectives. Thus, recovering them all is a computationally expensive process and requires an exhaustive exploration of the complete state space. To address this problem, we use Pareto Conditioned Networks~(PCN), a method that uses a single neural network to encompass all non-dominated policies \cite{reymond2022pcn}. The key idea behind PCN is to use supervised learning techniques to improve the policy instead of resorting to temporal-difference learning. % This eliminates the moving-target problem \cite{schmidhuber2019}, resulting in stable learning algorithm.

One major challenge of using neural networks as function approximators in RL is that the target (e.g., the optimal action of the policy) is not known in advance, as opposed to classical supervised learning where the ground-truth target is provided. As the behavior of the agent improves over time, the action used as target can change, often leading to hard-to-tune and brittle learners \cite{mnih2015,fu2019}.

Instead of trying to continuously improve the policy by learning actions that should lead to the highest cumulative reward, PCN flips the problem, by learning actions that should lead to any \emph{desired} cumulative reward (be it high or low). In this way, all past trajectories can be reused for supervision, since their returns are known, as well as the actions needed to reach said returns. We can thus train a policy that, conditioned on a desired return, provides the optimal action to reach said return. By leveraging the generalization properties of neural networks, we can accumulate increasingly better experience by conditioning on increasingly higher reward-goals.

Instead of using a neural network that takes a state $\mdpstate$ as input, PCN uses a single neural network that takes a tuple $\ltuple \mdpstate, \hat{h}, \mathbf{\hat{R}} \rtuple$ as input. $\mathbf{\hat{R}}$ represents the \emph{desired return} of the decision maker, i.e., the return PCN should reach at the end of the episode. $\hat{h}$ denotes the \emph{desired horizon} that expresses the number of timesteps that should be executed before reaching $\mathbf{\hat{R}}$. At execution time, both $\hat{h}$ and $\mathbf{\hat{R}}$ are chosen by the decision maker at the start of the episode. Then, at every timestep, the desired horizon is updated according to the perceived reward $\momdpreward_t$, $\mathbf{\hat{R}} \leftarrow \mathbf{\hat{R}} - \momdpreward_t$ and the desired horizon is decreased by one, $\hat{h} \leftarrow \hat{h}-1$.

PCN's neural network has an output for each action $\action_i \in \mathcal{A}$. Each output represents the confidence the network has that, by taking the corresponding action in $\mdpstate$, $\mathbf{\hat{R}}$ will be reached in $\hat{h}$ timesteps. We can draw an analogy with a classification problem where the network should learn to classify $\ltuple \mdpstate, \hat{h}, \mathbf{\hat{R}}\rtuple$ to its corresponding label $\action_i$.

Similar to classification, PCN requires a labeled dataset with training examples to learn a mapping from input to label. However, contrary to classification, the data present in the dataset is not fixed. PCN collects data from the trajectories experienced while exploring the environment. Thus, the dataset improves over time, as we collect increasingly better trajectories.

Concretely, the PCN algorithm implements an iterative process that repeatedly executes 3 main steps:
\begin{enumerate}
    \item Use the current policy to interact with the environment and generate trajectories (see Sec.~\ref{sec:pcn-policy-exploration}). Initially, a random policy is used to fill the dataset with random trajectories.
    \item Update the dataset to incorporate these new trajectories and prune the least interesting trajectories in terms of returns (see Sec.~\ref{sec:pcn-building-dataset}).
    \item Update the policy by training it on the updated dataset (see Sec.~\ref{sec:pcn-continuous}).
\end{enumerate}

\subsection{Building and updating the dataset}
\label{sec:pcn-building-dataset}

Given a trajectory composed of $T$ transitions $\ltuple \mdpstate_0, \action_0, \bm{r}_0, \mdpstate_1\rtuple, \dots, \ltuple \mdpstate_{T-1}, \action_{T-1}, \bm{r}_{T-1}, \mdpstate_T\rtuple$ we can compute, for each transition at timestep $t$, the future discounted return $\bm{R}_t = \sum_{i=t}^T \gamma^i\bm{r}_i$ and the leftover horizon $h_t = T-t$. Since for this trajectory executing action $a_t$ in state $s_t$ resulted in return $\mathbf{R}_t$ in $h_t$ timesteps, we add a datapoint with input $\langle s, \hat{h}, \mathbf{\hat{R}}\rangle = \langle s_t, h_t, \mathbf{R}_t \rangle$ and output $a=a_t$ to the dataset. In other words, when the observed return corresponds to the desired return in that state, then $a_t$ is the optimal action to take.

So that our neural network is trained on relevant transitions only, we keep a fix-sized dataset. Thus, adding a trajectory means we need to remove another one. Our aim is to keep trajectories that span different parts of the objective space, such that we can explore as many types of trade-offs as possible. However, to improve the trade-offs we currently have in our dataset, we need to only keep solutions that are close to our current estimate of the Pareto front. Thus, we assign a priority for each trajectory in our dataset that combines two metrics: the \emph{crowding distance}~\cite{deb2000}, which measures the distance of a solution with its closest neighbors, and the \emph{$\ell^2$-norm} with the closest non-dominated solution, which indicates how close the solution is from our current estimate of the Pareto front.

The dataset is then updated by only keeping the trajectories with the top-$N$ priorities, where $N$ is the size of the dataset.

\subsection{Generating trajectories with the current policy}
\label{sec:pcn-policy-exploration}

In the previous section, we explain how PCN updates the dataset given a trajectory. In this section, we explain how PCN produces said trajectory.

It is unrealistic to expect PCN to reliably produce trajectories with high-valued desired returns when it has only been trained on datapoints originating from random trajectories. Rather, we expect PCN to produce trajectories with returns in the range of the ones from the current training data. Therefore, if we obtain trajectories reaching high returns, PCN will be able to confidently return high-return policies.

PCN leverages the fact that, due to the generalization capabilities of neural networks, the policies obtained from the network will still be reliable even if the desired return is marginally higher than what is present in the training data. In fact, they will perform similar actions to those in the training data, but lead to a higher return. Thus, we incrementally condition the network on increasingly higher returns, to obtain trajectories that extend the boundaries of PCN's current coverage set.

More precisely, we randomly select a non-dominated return $\mathbf{R}_{nd}$ and its corresponding horizon $\hat{h}$ from the dataset. By randomly picking a non-dominated return from the entire coverage set we ensure equal probability to improve each part of the objective space. To generate trajectories that improve upon $\mathbf{R}_{nd}$, we randomly select an objective $o$ and increase $R_{nd,o}$ by a sample from the uniform distribution $\mathcal{U}(0, \sigma_o)$, where $\sigma_o$ is the standard deviation for the selected objective, using all non-dominated returns from the trajectories in the dataset. This then becomes our desired return $\bm{\hat{R}}$. By restricting the improvement to at most $\sigma_o$, $\mathbf{\hat{R}}$ stays in the range of achievable returns. By only modifying one objective at a time, the changes to the network's input compared to the training data are kept at a minimum.

\subsection{Training the network for continuous actions}
\label{sec:pcn-continuous}

PCN trains the network as a classification problem, where each class represents a different action. Transitions $x= \langle \mdpstate_t, h_t, \momdpreturn_t \rangle, y=\action_t$ are sampled from the dataset, and the ground-truth output $y$ is compared with the predicted output $\hat{y}=\pi(\mdpstate_t, h_t, \momdpreturn_t)$. The predictor (i.e., the policy) is then updated using the cross-entropy loss function \cite{shore1980axiomatic}:
\begin{equation}
    H = -\sum_{\action \in \mathcal{A}}{y_{\action} \log \pi(\action|\mdpstate_t,h_t,\momdpreturn_t)}
\end{equation}
where $y_\action = 1$ if $\action = \action_t$ and $y_\action = 0$ otherwise.

While the original PCN algorithm is designed for MOMDPs with discrete actions-spaces, the problem we tackle (see Sec.~\ref{sec:sars-cov2-momdp}) is defined in terms of a continuous action-space. We thus extend PCN to the continuous action-space setting.
We change the output of the neural network such that there is a single output value for each dimension of the action-space. Since the actions should be bound in the domain of possible actions ($[0,1]$ in the case of \momdpname, see Sec.~\ref{sec:sars-cov2-momdp}), we apply a sigmoid non-linearity function on each output, as the output of the sigmoid function is bound in $[0,1]$.
As such, we have a regression problem instead of a classification problem, as the labeled dataset now uses continuous labels $y = \mdpaction_t$ instead of categories. We thus use a Mean Squared Error (MSE) loss to update the policy:

\begin{equation}
    MSE = \frac{1}{|\mathcal{A}|}\sum_{a \in \mathcal{A}}{(\hat{y}_a - y_a)^2}
\end{equation}

Learning the full set of Pareto-efficient policies $\Pi^*$ requires that the policies $\pi^* \in \Pi^*$ are deterministic stationary policies. If stochastic policies are permitted, the set of optimal policies corresponds to the convex part of the Pareto front~\cite{roijers2013survey}. However, we argue that, in the context of mitigation policies, deterministic policies are required, as the population needs to be informed in advance of imposed measures. Thus, we use the output $\mathbf{\hat{y}}$, which is deterministic, as action at execution time. However, PCN improves its policy through exploration, by continuously updating its dataset with better trajectories. Thus, at training time, we use a stochastic policy by adding random noise to the action \cite{lillicrap2015continuous}: 
\begin{equation}
    \mdpaction_t = \pi(\mdpstate_t, h_t, \momdpreturn_t) + \eta s \text { with } s \sim \mathcal{N},
\end{equation}

where $\mathcal{N}$ is the standard Normal distribution and $\eta$ is a hyper-parameter defining the magnitude of noise to be added.

\subsection{Coping with stochastic transitions}
\label{sec:pcn-stochastic}

PCN trains its policy on a dataset that is collected by executing trajectories. It assumes that reenacting a transition from the dataset leads to the same episodic return. When the transition function $\mathcal{T}$ of the MOMDP is deterministic, the whole trajectory can be faithfully reenacted, which guarantees the same return. Combined with the fact that PCN's policy is deterministic at execution time, conditioning the policy on a target episodic return is equivalent to conditioning it on the V-value $\mathbf{V}$.

However, when $\mdptransition$ is stochastic this can no longer be guaranteed. To mitigate this, we add a limited amount of random noise to $\mathbf{R}_t$ when performing gradient descent, which reduces the risk of overfitting~\cite{zur2009noise}. Moreover, while the \momdpname\ model is stochastic, the variation is entirely due to the sampling of the binomial distributions in the binomial-chain. While this variation accumulates over time, the time window we consider for each timestep (i.e., one week) is short enough that the accumulation remains bounded. Thus, the possible next-states resulting from a state-action pair are similar to each other. This allows PCN to compensate if $\momdpreward_t = \momdprewardfn(\mdpstate_t, \mdpaction_t, \mdpstate_{t+1})$ is worse than expected.

Although we use a stochastic model to cope with the uncertainty of the outcome of the outbreak, it is possible to deterministically evaluate the set of ordinary differential equations that define the model. We assess the validity of our approach by executing PCN on this deterministic variant, and observing similar performance as with the \momdpname\ model. We show the results in Sec.~\ref{sec:cs-ode-vs-binomial} of the Appendix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysing COVID-19 deconfinement policies}
\label{sec:experiments}
Our goal is to use PCN to learn deconfinment strategies in the \momdpname\ environment. We aim to learn policies that balance between the epidemiological objective of minimising the attack rate (i.e., $\mdprewardfn_\text{ARH}$ for hospitalisation and $\mdprewardfn_\text{ARI}$ for infection) and the social burden (i.e., $\mdprewardfn_\text{SB}$) experienced by the population population due to the implement mitigation measures. To this end, we consider two cases for the vectorial reward functions $[\mdprewardfn_\text{ARH}, \mdprewardfn_\text{SB}]$ and $[\mdprewardfn_\text{ARI}, \mdprewardfn_\text{SB}]$, to learn and analyse policies under different targets with respect to the considered attack rate.

To conduct this analysis, we apply our extension of PCN for continuous action-spaces on the \momdpname\ model. As explained in Sec.~\ref{sec:pcn-stochastic}, we extend PCN for environments with stochastic transitions.

Conform to \citet{abrams2021modelling}, the simulation starts on the 1st of March 2020, by seeding a number of infections in the population. Two weeks later, on the 14th of March, the Belgian government initiated a full lockdown. This is implemented by fixing the actions $p_w, p_s, p_l$ to $0.2, 0, 0.1$ respectively. This lockdown ended on the 4th of May 2020, at which point the government decided on a multi-phase exit strategy to incrementally reduce teleworking, reopen schools and allow leisure activities, such as the re-opening of bars and the cultural sector. It is from this day onward that PCN aims to learn policies for diverse exit strategies, compromising between the total number of daily hospitalizations and the total number of contacts lost as a proxy for social burden. The simulation lasts throughout the summer school holidays, from 01/07/2020 to 31/08/2020. Schools are closed during the school holidays, which is simulated by setting $p_s = 0$, regardless of the corresponding value outputted by the policy, i.e., during periods of school closure $p_s$ is ignored.

% https://nl.wikipedia.org/wiki/Maatregelen_tijdens_de_coronacrisis_in_Belgi%C3%AB
We draw the analogy with the multi-phase exit strategy established by the Belgian government and the restriction on the number of action-changes imposed by the \momdpname's budget $\budget$. Indeed, on the 11th of May 2020, exactly one week after the end of the lockdown, stores and certain companies were allowed to reopen, under strict conditions. This corresponds to altering $p_w$ in our MOMDP. One week later, on 18th of May, primary and secondary schools reopened for limited sized class-groups, and the cultural sector reopened partially. This is equivalent to increasing $p_s$ and $p_l$. Further changes of restrictions occured on the 8th of June, 1st, 9th and 25th of July. Thus, we argue that, with a limited budget, we can achieve realistic policies. In our experiments, we consider budgets of 2 to 5, as these closely relate to the number of changes that occured until the end of the summer holidays of 2020. As an upper bound, we also consider a no-limit budget setting.

To evaluate the quality of the policies learned by PCN, we compare PCN to a baseline. This baseline consists of a set of 100 fixed policies, that iterate over all the possible social restriction levels, with values ranging between 0 and 1. Concretely, each policy uses a fixed proportional reduction $p_w=p_l=p_s=u, u\sim\mathcal{U}(0, 1)$ throughout the episode. In other words, the fixed policies directly operate in a fine-grained manner on the whole contact reduction function $\hat{C}$. This allows us to obtain a strong baseline for potential exit strategies over the objective space. We note that while such fixed policies are a feasible approach, they do not scale well in terms of action and objective spaces and they will not be able to provide an adaptive restriction level, which is what we aim to provide using PCN.

All experiments are averaged over 10 runs. The hyper-parameters and the neural network architecture can be found in Sec.s~\ref{sec:pcn-hyperparameters} and Sec.~\ref{sec:pcn-architecture} of the Appendix, respectively.


\subsection{Learned coverage set}

\begin{figure}
    \centering
    \input{fig/arh_binomial_both}
    \caption{The Pareto front of policies discovered by PCN using \momdpname, showing the different compromises between the number of hospitalizations and the number of lost contacts. On the right, we show, for each budget setting (colored, subscript indicates budget) the coverage set learned by the best performing run. On the left, we show an interpolated average of the coverage sets learned by the different runs, with the shaded regions corresponding to the standard deviation. For comparison, the basline is displayed on both plots (in black). As the budget increases, so does the size of the coverage set learnt by PCN. Changes are most noticeable in the less restrictive trade-offs in terms of social burden.}
    \label{fig:pf-binomial-arh-budgets}
\end{figure}

\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{0.5em} % for the horizontal padding
    {\renewcommand{\arraystretch}{1.2}% for the vertical padding
    \input{tab/arh/binomial}
    }
    \caption{Evaluation metrics for the coverage sets comparing hospitalizations with social burden. In general, an increase of budget results in a better coverage set. Training on infections (ARI) still provides a competitive coverage set in terms of hospitalizations. All PCN coverage sets outperform the baseline.}
    \label{tab:coverage-set}
  \end{table}

We learn a coverage set (see Fig.~\ref{fig:pf-binomial-arh-budgets}) that ranges from imposing minimal restrictions to enforcing many restrictions. In Fig.~\ref{fig:pf-binomial-arh-budgets}, we display on the right the coverage set of the best-performing run in terms of hypervolume, for each budget setting. On the left, we show an interpolated average of the coverage sets learned by the different runs.

Regardless of the imposed budget, we notice that the coverage sets discovered by PCN almost completely dominate the coverage set of the baseline, demonstrating that there are better alternatives to the fixed policies. This is most evident in the compromising policies, where one has to carefully choose when to remove social restrictions while at the same time minimizing the impact on daily new hospitalizations. In these scenarios, PCN learns policies that drastically reduce the total number of new hospitalizations (e.g., more than $20000$) for the same social burden. We analyse the executions of such policies in Fig.~\ref{fig:policy-execution} (middle plot), that shows a flattened hospitalization curve, with a gradual increase of social freedom during the school holidays such that the curve of the epidemic is flattened and gradually decreases over time.

Interestingly, we notice that the most restrictive policy (i.e., the one that prioritizes hospitalizations over social burden, see Fig.~\ref{fig:policy-execution}, bottom plot) still starts to gradually increase $p_w$ and $p_l$ from the end of July onward. This is because by then, the epidemic has mostly faded out, and it is safe to reduce social restrictions. The timing of this reduction is important as reducing restrictions too soon can lead to a new wave. PCN learns the impact of its decisions over time, and correctly infers the timing at which restrictions can be safely lifted.

Finally, the top plot shows that, without imposing social restrictions, the number of hospitalisations peaks on the 15th of June. By the beginning of July, the epidemic has spread out over the majority of the population \todo{epi friends: can we mention here that we reached herd immunity?}, and the number of admissions at the hospital has been reduced to a fraction of the number of hospitalisations at the peak. Thus, without social restrictions, we do not take advantage of the natural decrease of social contacts due to school holidays, as a significant proportion of the population has already been infected before the start of the holidays.

\subsection{Impact of budgets on the coverage set}

\begin{figure}
    \centering
    \input{fig/budget_5}
    \caption{Selection of policies learned by $\text{PCN}_{b=5}$, from most restrictive in terms of social burden (top) to least restrictive (bottom). The x-axis represents the time, starting from the end of the lockdown, on the 4th of May, until the end of the school holidays, on the 1st of September. Since the lockdown is simulated before the start of the exit strategy, the start-state differs for each episode (i.e., the hospital already contains infected individuals). The left y-axis represents the number of individuals affected by the epidemic. The full-lined plots represent the number of individuals admitted into the hospital, ICU and deceased between the last timestep and the current one. The plot showing the newly deceased individuals closely relates to the ICU admissions. The right y-axis represents the proportional reduction in effect, with 1 meaning a business-as-usual policy, and 0 meaning a complete suppression of social contacts. The dotted-lined plots represent the proportional reductions for the work, leasure and school environments. We note that the school reduction automatically goes to 0 at the start of the school holidays.
    }
    \label{fig:policy-execution}
\end{figure}

Fig.~\ref{fig:pf-binomial-arh-budgets} demonstrates that the budget impacts the learnt coverage set. In general, an increase of budget is associated with an increasingly better coverage set, as policies learned using a higher budget dominate the ones learned with a lower budget. This is to be expected, as a higher budget gives the agent more freedom to change its actions as the epidemic progresses. %However, we also notice that this increase of coverage sets is not necessarily gradual. % Between the run with a budget of 2 and budget of 3, the difference in coverage set seems marginal. \todo{insight}.

Moreover, we observe that the difference is concentrated around the less restrictive policies in terms of social burden. We postulate that this region contains the most complex policies, as these try to maintain as much social freedom as possible, while containing the number of hospitalisations. In these cases, the timing of the actions coincide with the timing and duration of the peak of the epidemic, and a higher budget allows for more fine-grained control to manage this timing. To confirm this, we select, for each budget setting, the solution where the difference in performance is most noticeable, corresponding to the solutions with a total number of hospitalisations around $80000$ (which is in the middle of the range of possible hospitalisations, as can be seen in Fig.~\ref{fig:pf-binomial-arh-budgets}). We plot the execution of the corresponding policies in Fig.~\ref{fig:pe-binomial-arh-budgets} and analyse their impact in terms of social burden. First, we observe that the lower-budget policies are unable to reduce the social restrictions past the peak of the epidemic. In contrast, the setting with no budget restrictions meticulously controls the restrictions as the epidemic progresses, completely removing restrictions by the end of the wave. Second, we note that the policy with a budget of 5 resembles the execution of the one without restrictions. However, due to its budget, the policy is unable to progressively reduce the restrictions and instead resorts to a halfway compromise. Compared to this specific region of the coverage set, the difference in performance between different budget settings seems marginal around the extremas. At the extremas, the policies are less complex (e.g., business-as-usual, resulting in the same action executed throughout the episode) and are thus less impacted by the budget restrictions.

Finally, we observe that, while the extremas deliver similar trade-offs for any of the chosen budgets, these trade-offs differ for the setting without budget restrictions. Indeed, in this setting, PCN does not learn the most extreme policies with respect to restrictions, even though there are no constraints on the action-set. As explained in Sec.~\ref{sec:pcn-policy-exploration}, PCN searches for increasingly better solutions using a stochastic policy. Thus, at every timestep, the action can change compared to the previous one. Continuously outputting the same action (e.g., no social restrictions) becomes a complicated task. In comparison, for the settings with a limited budget, the action stays the same as the previous one once the budget has been spent. As such, it is easier to learn the most extreme policies. Thus, in the specific case where we have an unlimited budget, the freedom of action actually hinders PCN's search, for certain regions of the reward-space.

\begin{figure}
    \centering
    \input{fig/all_budgets}
    \caption{Execution of the policies attaining a number of hospitalisations around $80000$, for different budgets. From top to bottom we display the policy executions with budget 2,3,4,5 and no-limit, respectively. We notice that the lower-budget policies are unable to reduce the social restrictions past the peak. The setting without budget restrictions finely controls the restrictions as the epidemic progresses, completely removing restrictions by the end of the wave. Finally, there is no consensus on which social environment to restrict most: certain policies provide similar restrictions for $p_w$ and $p_l$, while others impose harsher restrictions on one social environment than the other.
    }
    \label{fig:pe-binomial-arh-budgets}
\end{figure}

\subsection{$R_\text{ARH}$ versus on $R_\text{ARI}$}
\label{sec:arh-vs-ari}

Next, we assess the difference in coverage sets when optimizing on $\mdprewardfn_\text{ARH}$ versus $\mdprewardfn_\text{ARI}$. Although these reward functions have a different scale (there are more infected persons than hospitalised ones), our experiments show that infections and hospitalizations are tightly correlated. This is expected, as during the initial phase of the epidemic, limited immunity was present in the population (i.e., limited natural immunity and no vaccines), which induces a tight coupling between infection and hospitalisation cases. This is confirmed in Table~\ref{tab:coverage-set}. In this table, we show the different performance metrics (hypervolume, $I_\varepsilon$, $I_{\varepsilon-mean}$) with respect to the objectives $[\mdprewardfn_\text{ARH}, \mdprewardfn_\text{SB}]$. The table is split in two parts. The left-side shows the different performance metrics, for PCN using $[\mdprewardfn_\text{ARH}, \mdprewardfn_\text{SB}]$ as optimization criterions. The right-side shows the same performance metrics, but with PCN using $[\mdprewardfn_\text{ARI}, \mdprewardfn_\text{SB}]$ as optimization criterions.

Even with the $\mdprewardfn_\text{ARI}$, the increased budget shows an increase in hypervolume in terms of hospitalisations. Moreover, those hypervolumes are close to the ones trained on $\mdprewardfn_\text{ARH}$, indicating that their coverage sets are similar. However, regardless of the imposed budget, the hypervolumes are slightly worse. This is to be expected, since those experiments are not directly optimised on $\mdprewardfn_\text{ARH}$. We draw a similar conclusion for $I_\varepsilon$: for budgets 2, 3 and 5, the difference between the worst-performing policy for the $\mdprewardfn_\text{ARI}$ variant and the $\mdprewardfn_\text{ARH}$ is less than $0.01$, indicating less than $1\%$ difference in return values between the two variants when comparing their worst-performing policy. As an exception, we notice that PCN without budget restrictions results in better performance across every metric for the $\mdprewardfn_\text{ARI}$ variant. Still, due to the high standard deviation of the unlimited budget, $\mdprewardfn_\text{ARH}$ setting, we do not believe this difference is meaningful. Thus, we could optimise on the attack rate of hospitalisations with $\mdprewardfn_\text{ARI}$. As there is a 2 week delay for hospitalisations, this would facilitate learning policies to react to unexpected changes earlier than using $\mdprewardfn_\text{ARH}$, given that a good proxy to the actual number of infections was available (e.g., due to a scale up of PCR testing, as was the case after the first lockdown).

\subsection{Robustness of policy executions}
\label{sec:pcn-robustness}

\begin{table}
    \centering
    \setlength{\tabcolsep}{0.5em} % for the horizontal padding
    {\renewcommand{\arraystretch}{1.2}% for the vertical padding
    \input{tab/arh/arh_binomial_robustness}
    }
    \caption{Comparing the difference in the desired return provided to PCN and the actual return PCN obtained when executing its policy. We see that, regardless of the setting, the learned policy faithfully receives a return similar to its desired return.}
    \label{tab:pcn-robustness}
\end{table}

The dataset of trajectories that PCN is trained on is pruned over time to keep only the most relevant trajectories. The returns of these trajectories are used in Fig.~\ref{fig:pf-binomial-arh-budgets} to visualize the learned coverage set. Each of these returns can be used as desired return for policy execution. We now assess the robustness of the executed policies, by comparing the return obtained after executing the policy with the corresponding target return. For each run, we execute the policy $10$ times and compute the $I_\varepsilon$ and $I_{\varepsilon-mean}$ metrics with respect to the coverage set learned during the run. We show that the executed policies reliably obtain returns that are similar to the desired return used to condition PCN.

Results are shown in Table~\ref{tab:pcn-robustness}. The $I_\varepsilon$ indicators shows that, regardless of the budget, the decision maker will lose at worst a $0.050$ normalized return in any of the objectives. On average, it will lose $0.010$ normalized returns, i.e., on average, the return obtained by executing a policy will either result in an additional $1441$ hospitalisations than expected, or result in $12$ additional social contacts lost. Moreover, we emphasize that the learned coverage set contains the non-dominated returns encountered over the whole training procedure. Since the \momdpname\ model is stochastic, for multiple executions of the same policy, the executions kept in the coverage sets are the ones for which the samples from the binomial-chain resulted in a better progression of the epidemic than average. Thus, we expect our policy-executions to be close to the target selected from the coverage set, but not exactly on target. Based on this analysis, we conclude that the policies trained by PCN are robust and produce returns as close as possible from their chosen target.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
Reinforcement learning (RL) has been used in conjunction with epidemiological models to learn policies to limit the spread of diseases and predict the effects of possible mitigation strategies \cite{probert2019context,libin2018bayesian,libin2020}.
 
RL and Deep RL have been used extensively as a decision making aid to reduce the spread of COVID-19. For example, to learn effective mitigation strategies \cite{ohi2020exploring}, to learn efficacy of lockdown and travel restrictions \cite{kwak2021covid} and to limit the influx of asymptomatic travellers \cite{bastani2021efficient}.

Multi-objective methods have also been deployed to learn optimal strategies to mitigate the spread of COVID-19. Wan et al. \cite{wan2021multi} implement a model-based multi-objective policy search method and demonstrate their method on COVID-19 data from China. Given that this method is model-based, a model of the transition function must be learned by sampling from the environment. The method proposed by Wan et al. \cite{wan2021multi} only considers a discrete action space which limits the application of their algorithm. Wan et al. \cite{wan2021multi} use linear weights to compute a set of Pareto optimal policies. However, methods which use linear weights can only learn policies on the convex-hull of the Pareto front \cite{vamplew2008limitations}, therefore the full Pareto front cannot be learned. We note that the method proposed by Kompella et al. \cite{kompella2020reinforcement} considers multiple objectives. However, the objectives are combined using a weighted sum with hand-tuned weights which are determined by the authors. The weighted sum is applied by the reward function and a single objective RL method is used to learn a single optimal policy.
In contrast to previous work, our approach makes no assumptions regarding the scalarisation function of the user and is able to discover Pareto fronts of arbitrary shape.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and discussion}
Making decisions on how to mitigate epidemics has important ethical implications with respect to public health and societal burden. In this regard, it is crucial to approach this decision making from a balanced perspective, to which end we argue that multi-objective decision making is essential. In this work, we establish a novel approach, i.e., an expert system, to study multi-faceted policies, and this approach shows great potential to study future epidemic mitigation policies. We are aware of the ethical implications that expert systems have on the decision process and we make the disclaimer that all results based on the expert system that we propose should be carefully interpreted by experts in the field of public health, and in a broader context that encompasses health economics, well-being and education \todo{epi friends, other things to add here? To make sure we are exhaustive}. We note that the work in this manuscript was conducted by a interdisciplinary consortium that includes computer scientists and scientists with a background public health, epidemiology and bio-statistics.

In this work, we focus on the clinical outcomes of intervention strategies and use the reduced contacts as proxy for social burden. This could be extended into more formal health economic evaluations, by designing reward functions that explicitely consider distinct health economic principles. The COVID-19 pandemic demonstrates the broad impact of infectious diseases on sectors other than health care. This stresses the need to capture a societal and thus multi-objective perspective in the decision making process on public health and health care interventions. Our learned policies confirm this, showing that focusing solely on reducing the number of hospitalizations results in taking drastic measures -- more than a thousand social interactions lost per person over the span of 4 months -- that may have a long-lasting impact on the population.

Although we use an age-structured compartment model, with social contact matrices to model social interactions, this remains a model that evaluates the progression of the pandemic as an aggregated process over the population. Individual-based models could provide more detailed and localized policies, potentially further improving the quality of possible trade-offs, and would provide an interesting avenue for future work. However, due to the computational cost of simulating such models, and the number of interactions required by reinforcement learning in general, this remains a challenging problem, that will require fundamental research to improve the sample efficiency of  multi-objective reinforcement learning algorithms.

While we are able to interpret and analyse the obtained policies and their corresponding trade-offs, as we can plot the Pareto front for two objectives, this approach cannot be used for problems with more objectives, which will be necessary to cover reward functions that cover distinct health economic principles. To facilitate this kind of research, new algorithms are necessary to enable reinforcement learning in many-objective contexts and to interprete the learnt policies.

In this work, PCN is able to cope with model stochasticity, as the stochasticity is limited. In settings where the stochasticity is more pronounced, e.g., when designing policies to control the initial outbreak of an epidemic, further methodological extensions are warranted.

Finally, in this work we studied policies that aim to balance social burden and hospitalizations. Yet, the methodology that we propose shows promise to address a wide variety of public health challenges, such as balancing the number of lost schooldays with respect to the attack rate of infections in schools \cite{torneri2021controlling}, contact tracing effort compared to the impact of such policies \cite{willem2021impact}, the impact of antivirals on the epidemic while balancing the likelihood for resistance mutations to emerge \cite{torneri2020prospect}, and to balance the cost of universal testing and its impact on an emerging epidemic \cite{libin2021assessing}.

To conclude, we show that multi-objective reinforcement learning can be used to learn a wide set of high-quality policies on real-world problems, providing the decision maker with insightful and diverse alternatives, and showing the impact of extreme measures. Furthermore, we show that action budgets can act as a regulariser that facilitates learning realistic policies that can be easily conveyed to decision makers.

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}
\counterwithin{equation}{section}

\section{Stochastic Compartmental Model}
\label{sec:binomial-model}
In this work we utilise the compartmental model proposed by \citet{abrams2021modelling} and extend the model to a multi-objective Markov decision process (MOMDP). Fig.~\ref{fig:compartment_model} shows a visual depiction of the compartment model. The flows of the deterministic model are defined by a set of ordienary differential equations, outlined in Sec.~\ref{sec:sars-cov2-momdp_model}.

Intervening in the spread of the virus by, for example, reducing social contacts or government interventions introduces uncertainty in the further course of the outbreak. Therefore, to understand how this uncertainty affect the spread of the disease we introduce a stochastic component model which can model the uncertainty generated by interventions in social contacts. 

By formulating the set of differential equations defined in Sec.~\ref{sec:sars-cov2-momdp_model}, as a chain-binomial, we can obtain stochastic trajectories from this model \cite{bailey1975mathematical}. A chain-binomial model assumes a stochastic model where infected individuals are generated by some underlying probability distribution. For the stochastic model we consider a time interval $(t, t +h]$, where $h$ is defined as the length between two consecutive time points. Similar to \citet{abrams2021modelling}, in this work we set $h = \frac{1}{24}$. \citet{abrams2021modelling} define the set of differential equations as a chain binomial as follows:

\begin{align*}
\textbf{S}_{t+h} (k) & = \textbf{S}_{t} (k) - \textbf{E}_{new, t+h}(k),\\
%
\textbf{E}_{t+h}(k) & = \textbf{E}_{t}(k) + \textbf{E}_{new, t+h}(k) - \textbf{I}^{presym}_{new, t + h} (k),\\
%
\textbf{I}^{presym}_{t+h}(k) & = \textbf{I}^{presym}_{t}(k) + \textbf{I}^{presym}_{new, t + h}(k) - \textbf{I}^{asym}_{new, t + h}(k) - \textbf{I}^{mild}_{new, t + h}(k),\\
%
\textbf{I}^{asym}_{t+h}(k) & = \textbf{I}^{asym}_{t}(k) + \textbf{I}^{asym}_{new, t + h}(k) - \textbf{R}^{asym}_{new, t + h}(k),\\
%
\textbf{I}^{mild}_{t+h}(k) & = \textbf{I}^{mild}_{t}(k) + \textbf{I}^{mild}_{new, t + h}(k) - \textbf{I}^{sev}_{new, t + h}(k) - \textbf{R}^{mild}_{new, t + h}(k),\\
%
\textbf{I}^{sev}_{t+h}(k) & = \textbf{I}^{sev}_{t}(k) + \textbf{I}^{sev}_{new, t + h}(k) - \textbf{I}^{hosp}_{new, t + h}(k) - \textbf{I}^{icu}_{new, t + h}(k),\\
%
\textbf{I}^{hosp}_{t+h}(k) & = \textbf{I}^{hosp}_{t}(k) + \textbf{I}^{hosp}_{new, t + h}(k) - \textbf{D}^{hosp}_{new, t + h}(k) - \textbf{R}^{hosp}_{new, t + h}(k),\\
%
\textbf{I}^{icu}_{t+h}(k) & = \textbf{I}^{icu}_{t}(k) + \textbf{I}^{icu}_{new, t + h}(k) - \textbf{D}^{icu}_{new, t + h}(k) - \textbf{R}^{icu}_{new, t + h}(k),\\
%
\textbf{D}_{t+h}(k) & = \textbf{D}_{t}(k) + \textbf{D}^{hosp}_{new, t + h}(k) + \textbf{D}^{icu}_{new, t + h}(k),\\
%
\textbf{R}_{t+h}(k) & = \textbf{R}_{t}(k) + \textbf{R}^{asym}_{new, t + h}(k) + \textbf{R}^{mild}_{new, t + h}(k) + \textbf{R}^{hosp}_{new, t + h}(k) + \textbf{R}^{icu}_{new, t + h}(k)
\end{align*}

where,

\begin{align*}
\textbf{E}_{new, t+h} & \sim \textit{Binomial} \left( \textbf{S}_{t} (k), p^{*}_t (k) = 1 - \{ 1 -  p^{*}_t (k)\}^{\textbf{I}_{t}} \right), \\
%
p^{*}_t (k) & = 1 - exp \left[ -h \sum^{K}_{k' = 1} \beta_{asym}(k, k')\{ \textbf{I}^{asym}_{t}(k')\} + \beta_{sym}(k, k') \{\textbf{I}^{mild}_{t}(k')+\textbf{I}^{sev}_{t}(k')\} \right],\\
%
\textbf{I}^{presym}_{new, t+h} (k)&  \sim Binomial\left( \textbf{I}^{presym}_{t}(k), 1 - exp(-hp(k)\theta) \right),\\
%
\textbf{I}^{mild}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{presym}_{t}(k), 1 - exp\left[-h \{1-p(k)\}\theta \right]) \right),\\
%
\textbf{I}^{sev}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{mild}_{t}(k), 1 - exp\{-h\psi(k)\}) \right),\\
%
\textbf{I}^{hosp}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{sev}_{t}(k), 1 - exp\{-h\phi_{1}(k)\omega(k)\}) \right),\\
%
\textbf{I}^{icu}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{sev}_{t}(k), 1 - exp\left[-h \{1-\phi_{1}(k)\}\omega(k) \right] \right),\\
%
\textbf{D}^{hosp}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{hosp}_{t}(k), 1 - exp\{-h\tau_{1}(k) \} \right),\\
%
\textbf{D}^{icu}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{icu}_{t}(k), 1 - exp\{-h\tau_{2}(k) \} \right),\\
%
\textbf{R}^{asym}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{asym}_{t}(k), 1 - exp\left(-h\delta_{2}(k) \right) \right), \\
%
\textbf{R}^{hosp}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{hosp}_{t}(k), 1 - exp\{-h\delta_{3}(k) \} \right),\\
%
\textbf{R}^{icu}_{new, t+h} (k) & \sim Binomial\left( \textbf{I}^{icu}_{t}(k), 1 - exp\{-h\delta_{4}(k) \} \right).
\end{align*}

Given \momdpname\ also calculates new hospitalisations, $\textbf{H}^{new}$, we define $\textbf{H}^{new}$ for the stochastic compartmental model as follows:
\[
\textbf{H}^{new}_{t+h}(k) = \textbf{H}^{new}_{t}(k) + \textbf{I}^{hosp}_{new, t+h} (k).
\]
For more details on this model and the chain-binomial representation of the differential equations, we refer the reader to the work of \citet{abrams2021modelling}.

To create a version of \momdpname\ with a stochastic transition function, $M$, we utilise the stochastic compartmental model outlined above. Given the transitions within the compartmental model are derived by an underlying probability distribution it is possible to utilise the stochastic compartmental model transitions for \momdpname. As previously outlined in Sec.~\ref{sec:sars-cov2-momdp_model} the contact matrix $\hat{C}$ applied the model state $s_{m}$ progresses the model and returns a new model state $s'_{m}$. Given the underlying model dynamics are governed in a probabilistic manner, the model returns $s'_{m}$ stochastically. Therefore, it is possible to use this process as a stochastic transition function, $M$, for \momdpname. 

\subsection{A Note on Model Parameters}
\label{sec:compartment-model-parameters}

The model is parameterised using the mean of the posteriors as reported by \citet{abrams2021modelling}.

The population size for each of the considered age groups was taken from the Belgian statistical agency STATBEL\footnote{\url{https://statbel.fgov.be/nl/themas/bevolking/structuur-van-de-bevolking\#figures}}. To initialise the model, we used the number of confirmed cases until 13 March 2020 \cite{abrams2021modelling}, as reported by the Belgian agency for public health Sciensano\footnote{\url{https://epistat.wiv-isp.be/covid/}}.

\subsection{Modelling interventions}
\label{sec:compliance-function}
In order to model different types of interventions, we follow \citet{abrams2021modelling}. Firstly, to consider distinct exit scenarios, we alter the social contact matrices to reflect a contact reduction in a particular age group. Secondly, we assume that compliance to the interventions is gradual and model this using a logistic compliance function. We use the logistic compliance function in function of time $t$,
%
\begin{equation}
c(t,t_I) = \frac{\exp(\beta^*_0 + \beta^*_1(t-t_I))}{1+\exp(\beta^*_0 + \beta^*_1(t-t_I))},
\end{equation}
%
where $t_I$ indicates the time the intervention started. We initialise $\beta^*_1$ to the value estimated in by Abrams et al. and choose $\beta^*_0=-5$, as an intercept to have $c(t)=0$ for $t=0$, in correspondence with Fig.~F2 in the Supplementary Information of \citet{abrams2021modelling}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional results}

\subsection{Comparison of coverage sets learned on ODE and Binomial models}
\label{sec:cs-ode-vs-binomial}

\begin{figure}[t!]
    \centering
    \input{fig/arh_binomial_ode}
    \caption{Comparison of learned coverage sets when PCN interacts with the ODE (i.e., deterministic) variant of the compartment model. We observe that, regardless of the chosen budget, PCN learns a similar coverage set on both variants of the model, indicating that it is able to cope with the stochasticity present in the Binomial variant.}
    \label{fig:pf-binomial-ode}
\end{figure}

The coverage sets displayed in Fig.~\ref{fig:pf-binomial-arh-budgets} correspond to PCN trained on the \momdpname\ model, which is stochastic. To show that PCN copes with the stochasticity of this model, we compare these coverage sets with the ones learned on the ODE model, which is deterministic.

Results are shown in Fig.~\ref{fig:pf-binomial-ode}. First, in a similar fashion as Fig.~\ref{fig:pf-binomial-arh-budgets}, we show the interpolated average coverage set for different budget setting, with PCN trained on the ODE model. We observe similar trends as for the Binomial model. Next, for each budget setting, we compare the interpolated average coverage set of the Binomial setting with the ODE setting. We observe similar coverage set, regardless of the budget setting, indicating that PCN is able to cope with stochastic transitions.

\subsection{Comparison of coverage sets learned on $\mathbf{R}_{\text{ARH}}$ and $\mathbf{R}_{\text{ARI}}$}
\label{sec:cs-arh-vs-ari}

In Sec.~\ref{sec:arh-vs-ari}, we show that we can learn all trade-offs between social burden and hospitalisations, while using the attack rate over infections as reward function.

Results are shown in Fig.~\ref{fig:pf-binomial-arh-ari}. We observe that the learned coverage sets are similar, regardless of the budget setting. Still, the coverage set of when trained on $\mathbf{R}_{\text{ARI}}$ is systematically dominated by the one when trained on $\mathbf{R}_{\text{ARH}}$. \todo{Please check: is this actually true?} While hospitalisations and infections are highly correlated, they differ in terms of age-groups. Older age-groups are more susceptible to be hospitalised after being infected, but they do not form the majority of the population. For trade-offs where infections and social burden need to be balanced, the proportional reductions target different social environments than for trade-offs balancing hospitalisations and social burden. For example, the work environment is majorly comprised of individuals with a more robust immune system, reducing the social contact in this environment greatly affects the number of infections, but has a lesser impact on the number of hospitalisations.

\begin{figure}[t!]
    \centering
    \input{fig/arh_binomial_ari}
    \caption{Comparison of learned coverage sets when PCN learns using the attack rate of infections $\mathbf{R}_{\text{ARI}}$.}
    \label{fig:pf-binomial-arh-ari}
\end{figure}

\subsection{Policy executions}

Depending on the budget, PCN learns a coverage set containing more than 150 different policies. To gain a better insight about their behavior, and how they differ from each other, we plot executions of each learned policy in Fig.s~\ref{fig:policy-executions-b2-0}-\ref{fig:policy-executions-b2-140}. The plots are displayed from the least restrictive policy in terms of social burden to the most restrictive one. Since the \momdpname\ model is stochastic, we show 10 executions of the same policy, on each plot.

\foreach \i/\j in {0/19, 20/39, 40/59, 60/79, 80/99, 100/119, 120/139, 140/153}{
    \begin{figure}
        \centering
        \plotPolicies{\i}{\j}{2}
        \caption{Execution of policies \i\ to \j, with a budget of 2.}
        \label{fig:policy-executions-b2-\i}
    \end{figure}
}

\foreach \i/\j in {0/19, 20/39, 40/59, 60/79, 80/99, 100/119, 120/139, 140/159, 160/160}{
    \begin{figure}
        \centering
        \plotPolicies{\i}{\j}{3}
        \caption{Execution of policies \i\ to \j, with a budget of 3.}
        \label{fig:policy-executions-b3-\i}
    \end{figure}
}

\foreach \i/\j in {0/19, 20/39, 40/59, 60/79, 80/93}{
    \begin{figure}
        \centering
        \plotPolicies{\i}{\j}{4}
        \caption{Execution of policies \i\ to \j, with a budget of 4.}
        \label{fig:policy-executions-b4-\i}
    \end{figure}
}

\foreach \i/\j in {0/19, 20/39, 40/59, 60/79, 80/99, 100/114}{
    \begin{figure}
        \centering
        \plotPolicies{\i}{\j}{5}
        \caption{Execution of policies \i\ to \j, with a budget of 5.}
        \label{fig:policy-executions-b5-\i}
    \end{figure}
}

\foreach \i/\j in {0/19, 20/39, 40/59, 60/79, 80/99, 100/116}{
    \begin{figure}
        \centering
        \plotPolicies{\i}{\j}{None}
        \caption{Execution of policies \i\ to \j, with no budget restriction ($b=\infty$).}
        \label{fig:policy-executions-binf-\i}
    \end{figure}
}

\subsection{Experiment parameters}
\label{sec:pcn-hyperparameters}

We used the same hyper-parameters across all experiments. Each experiment resulted in 10 independent trials. Finally, we performed a grid-search over possible hyper-parameter values. All hyper-parameters used and their possible values explored during grid-search are displayed in Table~\ref{tab:pcn-hyperparameters}.

\begin{table}[t]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        hyper-parameter & value & grid-search \\
        \hline
        learning rate & $0.001$ &  \\
        total training timesteps & $300000$  & \\
        batch size & $256$ & $256, 1024$ \\
        model updates & $50$ & \\
        episodes between updates & $10$ & \\
        ER size (in episodes) & $1000$ & $400, 500, 1000$ \\
        initial random episodes & $200$ & $50, 200$ \\
        exploration noise & $0.1$ & $0, 0.1, 0.2$ \\
        desired return noise & $0.05$ & $0, 0.05, 0.1, 0.2$ \\
        reward scaling & $[10000, 100]$ & \\
        \hline
    \end{tabular}
    \caption{The different hyper-parameters used by our extension of PCN. The right-most column also shows, when applicable, the different values tried during grid-search.}
    \label{tab:pcn-hyperparameters}
\end{table}

\begin{table}[h]
    \centering
    \input{tab_nn-architecture}
    \caption{The 4 different neural network architectures explored for our experiments. All the displayed results use the \texttt{dense-big} variant.}
    \label{tab:nn-architecture}
\end{table}

\subsection{Neural network architecture}
\label{sec:pcn-architecture}

Next to the hyper-parameter search, we also performed a grid search over 4 different neural network architectures. All the architectures have the same structure. We use a compartment embedding $sc_{emb}$, a social contact matrix embedding $sm_{emb}$ and a school-holidays embedding $sh_{emb}$ that take as inputs the compartment, the previous $p_w, p_s, p_l$ values (as they fully define the SCM $\hat{C}$) and a boolean flag for school holidays, respectively. All these embeddings have a same-sized embedding of 64, which are multiplied together to form the full state embedding. This state-embedding is used as input for another network, $s_{emb}$. Additionally, we use a common embedding $c_{emb}$ for the concatenation of the desired return and horizon. Finally, the results of $s_{emb}$ and $c_{emb}$ are multiplied together, before passing through a fully connected network $fc$ that has 3 outputs, one for $p_w, p_s, p_l$ respectively.

All the architectures of the different components are displayed in Table~\ref{tab:nn-architecture}. The variant used in all experiments is \texttt{dense-big}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
